<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <title>Difference between PreviewLayer, VideoDataOutput and MetadataOutput</title>
    <link rel="stylesheet" href="../../style.css">
    <script src="../../index.js"></script>
</head>
<body>
    <header>
        <nav class="corporate-nav">
            <div class="nav-container">
                <div class="title-header">
                    <h1 class="title">Difference between PreviewLayer, VideoDataOutput and MetadataOutput</h1>
                    <p class="subtitle">
                        In AVFoundation, these three often get confused because they all ‚Äúdeal with camera output,‚Äù but they serve very different roles in the capture pipeline.
                        <p>I‚Äôll explain them conceptually, then low-level, then give a when-to-use table, and finally a real barcode scanner example (which fits what you‚Äôve been working on).</p>
                    </p>
                </div>        
            </div>
        </nav>
    </header>

    <main>
        <section>
            <h2>1Ô∏è‚É£ AVCaptureVideoPreviewLayer (PreviewLayer)</h2>
            <p>What it is</p>
            <p>A CALayer subclass whose only job is to display live camera frames on screen.</p>
            <ul>
                <li>It does NOT give you image data</li>
                <li>It does NOT analyze frames</li>
                <li>It does NOT affect capture quality</li>
                <li>It is GPU-accelerated</li>
            </ul>

            <p>What it connects to</p>
            <ul>
                <li>
                    Attached directly to an AVCaptureSession
                    <pre><code>
AVCaptureVideoPreviewLayer *preview =
    [AVCaptureVideoPreviewLayer layerWithSession:session];
                    </code></pre>
                </li>
            </ul>

            <p>What happens internally</p>
            <pre><code>
Camera Sensor
   ‚Üì
ISP (Image Signal Processor)
   ‚Üì
AVCaptureSession
   ‚Üì
PreviewLayer ‚Üí Metal/OpenGL ‚Üí Screen
            </code></pre>

            <ul>
                <li>Frames go directly to the GPU</li>
                <li>Zero CPU involvement</li>
                <li>No pixel buffers exposed to your app</li>
            </ul>
            <p>Key properties</p>
            <ul>
                <li>videoGravity (aspectFill / aspectFit)</li>
                <li>connection.videoOrientation</li>
            </ul>
            <p>Typical use</p>
            <ul>
                <li>Camera viewfinder</li>
                <li>Barcode scanner camera UI</li>
                <li>Face camera preview</li>
            </ul>
        </section>

        <section>
            <h2>2Ô∏è‚É£ AVCaptureVideoDataOutput (VideoDataOutput)</h2>
            <p>What it is</p>
            <p>A raw frame delivery output that sends every video frame to your code.</p>
            <ul>
                <li>You receive CMSampleBufferRef</li>
                <li>CPU or GPU processing required</li>
                <li>Heavy if misused</li>
            </ul>
            <p>What it connects to</p>
            <ul>
                <li>Attached to an AVCaptureSession</li>
                <li>Requires a delegate</li>
            </ul>
            <pre><code>
AVCaptureVideoDataOutput *videoOutput = [[AVCaptureVideoDataOutput alloc] init];
[videoOutput setSampleBufferDelegate:self queue:videoQueue];
            </code></pre>

            <p>What happens internally</p>
            <pre><code>
Camera Sensor
   ‚Üì
ISP
   ‚Üì
AVCaptureSession
   ‚Üì
VideoDataOutput
   ‚Üì
CMSampleBuffer
   ‚Üì
CPU / CoreImage / Metal
            </code></pre>

            <p>You can do things like</p>
            <ul>
                <li>Convert to CVPixelBuffer</li>
                <li>Run Core ML</li>
                <li>Run OpenCV</li>
                <li>Custom image processing</li>
                <li>Save raw frames</li>
            </ul>
            <p>Performance impact</p>
            <p>‚ö†Ô∏è High</p>
            <p>You are now in the critical frame path.</p>
            <p>Typical use</p>
            <ul>
                <li>Machine learning</li>
                <li>Computer vision</li>
                <li>Custom filters</li>
                <li>Recording with custom encoding</li>
            </ul>
        </section>

        <section>
            <h2>3Ô∏è‚É£ AVCaptureMetadataOutput (MetadataOutput)</h2>
            <p>What it is</p>
            <p>A hardware-accelerated analyzer that extracts metadata from frames.</p>
            <p>It does not give you images ‚Äî only results.</p>
            <p>Supported metadata</p>
            <ul>
                <li>Barcodes (QR, EAN, Code 128, etc.)</li>
                <li>Faces</li>
                <li>Text (limited)</li>
                <li>Object metadata</li>
            </ul>

            <p>What it connects to</p>
            <ul>
                <li>Attached to an AVCaptureSession</li>
                <li>Delegate receives detected objects</li>
            </ul>

            <pre><code>
AVCaptureMetadataOutput *metadataOutput = [[AVCaptureMetadataOutput alloc] init];
[metadataOutput setMetadataObjectsDelegate:self queue:mainQueue];
metadataOutput.metadataObjectTypes = metadataOutput.availableMetadataObjectTypes;
            </code></pre>

            <p>What happens internally</p>
            <pre><code>
Camera Sensor
   ‚Üì
ISP
   ‚Üì
AVCaptureSession
   ‚Üì
MetadataOutput
   ‚Üì
Hardware Vision Engines
   ‚Üì
AVMetadataMachineReadableCodeObject
            </code></pre>

            <ul>
                <li>Uses Apple‚Äôs private vision hardware</li>
                <li>Much faster than VideoDataOutput</li>
                <li>No pixel buffers exposed</li>
            </ul>

            <p>Typical use</p>
            <ul>
                <li>Barcode scanning</li>
                <li>Face detection</li>
                <li>Lightweight recognition</li>
            </ul>
        </section>

        <section>
            <h2>4Ô∏è‚É£ Side-by-Side Comparison</h2>
            <table>
                <thead>
                    <tr>
                        <th>Feature</th>
                        <th>PreviewLayer</th>
                        <th>VideoDataOutput</th>
                        <th>MetadataOutput</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Displays camera</td>
                        <td>‚úÖ Yes</td>
                        <td>‚ùå No</td>
                        <td>‚ùå No</td>
                    </tr>
                    <tr>
                        <td>Provides image data</td>
                        <td>‚ùå No</td>
                        <td>‚úÖ Yes</td>
                        <td>‚ùå No</td>
                    </tr>
                    <tr>
                        <td>Provides detection results</td>
                        <td>‚ùå No</td>
                        <td>‚ùå No</td>
                        <td>‚úÖ Yes</td>
                    </tr>
                    <tr>
                        <td>CPU usage</td>
                        <td>üü¢ Very low</td>
                        <td>üî¥ High</td>
                        <td>üü° Low</td>
                    </tr>
                    <tr>
                        <td>GPU usage</td>
                        <td>üü¢ Efficient</td>
                        <td>üü° Optional</td>
                        <td>üü¢ Efficient</td>
                    </tr>
                    <tr>
                        <td>Real-time safe</td>
                        <td>‚úÖ Yes</td>
                        <td>‚ö†Ô∏è Careful</td>
                        <td>‚úÖ Yes</td>
                    </tr>
                    <tr>
                        <td>Delegate required</td>
                        <td>‚ùå No</td>
                        <td>‚úÖ Yes</td>
                        <td>‚úÖ Yes</td>
                    </tr>
                </tbody>
            </table>
        </section>

        <section>
            <h2>5Ô∏è‚É£ How They Are Commonly Used Together</h2>
            <p>Most camera apps use all three at once:</p>
            <pre><code>
AVCaptureSession
 ‚îú‚îÄ‚îÄ AVCaptureVideoPreviewLayer   ‚Üí UI
 ‚îú‚îÄ‚îÄ AVCaptureMetadataOutput      ‚Üí Barcode / Face detection
 ‚îî‚îÄ‚îÄ AVCaptureVideoDataOutput     ‚Üí (Optional) Custom processing
            </code></pre>
        </section>

        <section>
            <h2>6Ô∏è‚É£ Barcode Scanner Example (Best Practice)</h2>
            <pre><code>
// 1. Preview (UI)
self.previewLayer = [AVCaptureVideoPreviewLayer layerWithSession:self.session];

// 2. Metadata (barcode detection)
AVCaptureMetadataOutput *metadataOutput = [[AVCaptureMetadataOutput alloc] init];
[metadataOutput setMetadataObjectsDelegate:self queue:dispatch_get_main_queue()];
metadataOutput.metadataObjectTypes = @[AVMetadataObjectTypeEAN13Code];

// NO VideoDataOutput needed unless doing custom CV
            </code></pre>
            <p>üëâ Never use VideoDataOutput just to scan barcodes</p>
            <p>It‚Äôs slower and wastes battery.</p>
        </section>

        <section>
            <h2>7Ô∏è‚É£ Key Takeaways</h2>
            <ul>
                <li>PreviewLayer ‚Üí Show camera</li>
                <li>VideoDataOutput ‚Üí Process raw frames yourself</li>
                <li>MetadataOutput ‚Üí Let Apple detect things for you</li>
            </ul>
            <p>If you want, I can also:</p>
            <ul>
                <li>Explain why barcode detection stops after deletion (related to MetadataOutput throttling)</li>
                <li>Show why PreviewLayer orientation doesn‚Äôt match metadata coordinates</li>
                <li>Diagram threading & queues for each output</li>
            </ul>
        </section>
    </main>

    <footer>
        <p class="copyright">
            &copy; <span id="year"></span> Aether Corp. All rights reserved.
        </p>
    </footer>
</body>
</html>
